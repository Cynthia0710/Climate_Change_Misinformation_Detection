{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from raw data\n",
    "import json\n",
    "\n",
    "TRAIN_RUMOUR_JSON_PATH = 'data/raw/train.json'\n",
    "TRAIN_NONRUMOUR_TXT_PATH = 'data/raw/non-rumor.txt'\n",
    "DEV_JSON_PATH = 'data/raw/dev.json'\n",
    "TEST_JSON_PATH = 'data/raw/test-unlabelled.json'\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, 'r') as json_file:\n",
    "        data = json.loads(json_file.read())\n",
    "    return data\n",
    "\n",
    "def read_txt(path):\n",
    "    data = dict()\n",
    "    with open(path, 'r', encoding='utf-8') as txt_file:\n",
    "        num = 0\n",
    "        for line in txt_file:\n",
    "            text = dict()\n",
    "            text['text'] = line\n",
    "            data[num] = text\n",
    "            num = num + 1\n",
    "    return data\n",
    "                \n",
    "train_rumour_json_data = read_json(TRAIN_RUMOUR_JSON_PATH)\n",
    "train_nonrumour_txt_data = read_txt(TRAIN_NONRUMOUR_TXT_PATH)\n",
    "dev_json_data = read_json(DEV_JSON_PATH)\n",
    "test_json_data = read_json(TEST_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary and txt to list with same format\n",
    "def get_data_list(data):\n",
    "    data_list = []\n",
    "    for key, value in data.items():\n",
    "        text = value['text'].replace('\\n', ' ')\n",
    "        text = text.replace('\\u00A0', '')\n",
    "        data_list.append(text)\n",
    "    return data_list\n",
    "\n",
    "train_rumour_list = get_data_list(train_rumour_json_data)\n",
    "train_nonrumour_list = get_data_list(train_nonrumour_txt_data)\n",
    "dev_list = get_data_list(dev_json_data)\n",
    "test_list = get_data_list(test_json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train list and train label\n",
    "train_list = []\n",
    "train_label = []\n",
    "for i in train_rumour_list:\n",
    "    train_list.append(i)\n",
    "    train_label.append(1)\n",
    "for i in train_nonrumour_list:\n",
    "    train_list.append(i)\n",
    "    train_label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dev label\n",
    "def get_dev_label(dev_json_data):\n",
    "    dev_label = []\n",
    "    for key, value in dev_json_data.items():\n",
    "        dev_label.append(value['label'])\n",
    "    return dev_label\n",
    "\n",
    "dev_label = get_dev_label(dev_json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing (punctuation/special punctuation/single letter/lemma)\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def clean_text(text, tt, lemmatizer, punc, chinesepunc, letter, stopword_list):\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def lemma_process(word_token, lemmatizer):\n",
    "        word_token_pos_tag = nltk.pos_tag([word_token])\n",
    "        for word_token, pos_tag in word_token_pos_tag:\n",
    "            word_token_lower = word_token.lower()\n",
    "            wordnet_pos = get_wordnet_pos(pos_tag)\n",
    "            if wordnet_pos == None:\n",
    "                word_token_lemma = lemmatizer.lemmatize(word_token_lower, wordnet.NOUN)\n",
    "            else:\n",
    "                word_token_lemma = lemmatizer.lemmatize(word_token_lower, wordnet_pos)\n",
    "        return word_token_lemma     \n",
    "\n",
    "    def get_new_text(text, tt, lemmatizer, punc, chinesepunc, letter, stopword_list):\n",
    "        new_word_list = []\n",
    "        new_text = \"\"\n",
    "        for word in tt.tokenize(text.lower()):\n",
    "            word_only_string = re.sub (r'([^a-zA-Z ]+?)', '', word)\n",
    "            if (word_only_string not in punc) and (word_only_string not in chinesepunc) and (word_only_string not in stopword_list) and (word_only_string not in letter):\n",
    "                word_lemma = lemma_process(word_only_string, lemmatizer)\n",
    "                new_word_list.append(word_lemma)\n",
    "        new_text = \" \".join(new_word_list)\n",
    "        return new_text\n",
    "    \n",
    "    new_text = get_new_text(text, tt, lemmatizer, punc, chinesepunc, letter, stopword_list)\n",
    "    \n",
    "    return new_text\n",
    "    \n",
    "tt = TweetTokenizer()\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        \n",
    "punc = string.punctuation\n",
    "chinesepunc = ['“', '”', '‘', '’', '–', '—', '...', '‐', '\\u200b', '.\\u2009.\\u2009.', '\\uf0b7', '\\uf020', '\\u200e', '\\u2066', \n",
    "                '\\u2069', '..', '. .', '…']\n",
    "letter = ['a', 'b', 'c', 'd', \"e\", \"f\", 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', \n",
    "          'x', 'y', 'z'] \n",
    "stopword_list = set(stopwords.words('english'))\n",
    "\n",
    "preprocessed_train_list = []\n",
    "preprocessed_dev_list = []\n",
    "preprocessed_test_list = []\n",
    "\n",
    "for train in train_list:\n",
    "    new_train = clean_text(train, tt, lemmatizer, punc, chinesepunc, letter, stopword_list)\n",
    "    preprocessed_train_list.append(new_train)\n",
    "for dev in dev_list:\n",
    "    new_dev = clean_text(dev, tt, lemmatizer, punc, chinesepunc, letter, stopword_list)\n",
    "    preprocessed_dev_list.append(new_dev)\n",
    "for test in test_list:\n",
    "    new_test = clean_text(test, tt, lemmatizer, punc, chinesepunc, letter, stopword_list)\n",
    "    preprocessed_test_list.append(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 1), tokenizer=tt.tokenize)\n",
    "train_count_vector = cv.fit_transform(preprocessed_train_list)\n",
    "dev_count_vector = cv.transform(preprocessed_dev_list)\n",
    "test_count_vector = cv.transform(preprocessed_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer + TfidfTransformer = TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf = TfidfTransformer()\n",
    "train_tf_vector = tf.fit_transform(train_count_vector)\n",
    "dev_tf_vector = tf.transform(dev_count_vector)\n",
    "test_tf_vector = tf.transform(test_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dev label and test label\n",
    "def save_dev_label(prediction_list, path):\n",
    "    dev_total_labels_dict = dict()\n",
    "    num = 0\n",
    "    for dev_label in prediction_list:\n",
    "        label_dict = dict()\n",
    "        label_dict['label'] = dev_label\n",
    "        dev_total_labels_dict['dev-'+str(num)] = label_dict\n",
    "        num = num + 1\n",
    "    json_str = json.dumps(dev_total_labels_dict)\n",
    "    with open(path, 'w') as json_file:\n",
    "        json_file.write(json_str)\n",
    "        \n",
    "def save_test_label(prediction_list, path):\n",
    "    test_total_labels_dict = dict()\n",
    "    num = 0\n",
    "    for test_label in prediction_list:\n",
    "        label_dict = dict()\n",
    "        label_dict['label'] = test_label\n",
    "        test_total_labels_dict['test-'+str(num)] = label_dict\n",
    "        num = num + 1\n",
    "    json_str = json.dumps(test_total_labels_dict)\n",
    "    with open(path, 'w') as json_file:\n",
    "        json_file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "from sklearn import svm\n",
    "   \n",
    "DEV_LABEL_TFIDF_PREDICTION_PATH = 'data/dev/svm/prediction-tfidf-linear.json'\n",
    "TEST_LABEL_TFIDF_PREDICTION_PATH = 'data/test/svm/test-output-tfidf-linear.json'\n",
    "#DEV_LABEL_TFIDF_PREDICTION_PATH = 'data/dev/svm/prediction-tfidf-rbf.json'\n",
    "#TEST_LABEL_TFIDF_PREDICTION_PATH = 'data/test/svm/test-output-tfidf-rbf.json'\n",
    "\n",
    "svm_clf = svm.SVC(kernel='linear') #kernel='rbf'\n",
    "svm_clf.fit(train_tf_vector, train_label)\n",
    "prediction_dev = svm_clf.predict(dev_tf_vector)\n",
    "prediction_list = prediction_dev.tolist()\n",
    "save_dev_label(prediction_list, DEV_LABEL_TFIDF_PREDICTION_PATH)\n",
    "prediction_test = svm_clf.predict(test_tf_vector)\n",
    "prediction_list_test = prediction_test.tolist()\n",
    "save_test_label(prediction_list_test, TEST_LABEL_TFIDF_PREDICTION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58945\n"
     ]
    }
   ],
   "source": [
    "# get vocabulary size\n",
    "vocab_size = train_tf_vector.shape[1]\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feedforward-bow-input\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 150)               8841900   \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 20)                3020      \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 8,844,941\n",
      "Trainable params: 8,844,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4252 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "4252/4252 [==============================] - 6s 1ms/step - loss: 0.1566 - accuracy: 0.9511 - val_loss: 0.4359 - val_accuracy: 0.8800\n",
      "Epoch 2/5\n",
      "4252/4252 [==============================] - 6s 1ms/step - loss: 0.0154 - accuracy: 0.9967 - val_loss: 0.6007 - val_accuracy: 0.8900\n",
      "Epoch 3/5\n",
      "4252/4252 [==============================] - 6s 1ms/step - loss: 0.0054 - accuracy: 0.9981 - val_loss: 0.7339 - val_accuracy: 0.8800\n",
      "Epoch 4/5\n",
      "4252/4252 [==============================] - 6s 1ms/step - loss: 0.0123 - accuracy: 0.9967 - val_loss: 0.8782 - val_accuracy: 0.8400\n",
      "Epoch 5/5\n",
      "4252/4252 [==============================] - 6s 1ms/step - loss: 0.0035 - accuracy: 0.9986 - val_loss: 0.8238 - val_accuracy: 0.8600\n"
     ]
    }
   ],
   "source": [
    "# mlp\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy\n",
    "\n",
    "DEV_LABEL_COUNT_PREDICTION_PATH = 'data/dev/mlp/prediction-count-2layers150.json'\n",
    "TEST_LABEL_COUNT_PREDICTION_PATH = 'data/test/mlp/test-output-count-2layers150.json'\n",
    "DEV_LABEL_TFIDF_PREDICTION_PATH = 'data/dev/mlp/prediction-tfidf-2layers150.json'\n",
    "TEST_LABEL_TFIDF_PREDICTION_PATH = 'data/test/mlp/test-output-tfidf-2layers150.json'\n",
    "#DEV_LABEL_TFIDF_PREDICTION_PATH = 'data/dev/mlp/prediction-tfidf-1layer20.json'\n",
    "#TEST_LABEL_TFIDF_PREDICTION_PATH = 'data/test/mlp/test-output-tfidf-1layer20.json'\n",
    "#DEV_LABEL_TFIDF_PREDICTION_PATH = 'data/dev/mlp/prediction-tfidf-1layer150.json'\n",
    "#TEST_LABEL_TFIDF_PREDICTION_PATH = 'data/test/mlp/test-output-tfidf-1layer150.json'\n",
    "#TEST_LABEL_TFIDF_PREDICTION_PATH = 'data/test/mlp/test-output-tfidf-nopre.json'\n",
    "\n",
    "model = Sequential(name=\"feedforward-bow-input\")\n",
    "model.add(layers.Dense(150, input_dim=vocab_size, activation='relu'))\n",
    "model.add(layers.Dense(20, input_dim=150, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(train_tf_vector, train_label, epochs=5, verbose=True, validation_data=(dev_tf_vector, dev_label), batch_size=14)\n",
    "prediction_dev = model.predict_classes(dev_tf_vector)\n",
    "prediction_test = model.predict_classes(test_tf_vector)\n",
    "prediction_list = [int(numpy.round(x)) for x in prediction_dev]\n",
    "save_dev_label(prediction_list, DEV_LABEL_TFIDF_PREDICTION_PATH)\n",
    "prediction_list_test = [int(numpy.round(x)) for x in prediction_test]\n",
    "save_test_label(prediction_list_test, TEST_LABEL_TFIDF_PREDICTION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "DEV_LABEL_TFIDF_PREDICTION_PATH = 'data/final/dev/lr/prediction-tfidf.json'\n",
    "TEST_LABEL_TFIDF_PREDICTION_PATH = 'data/final/test/lr/test-output-tfidf.json'\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_tf_vector, train_label)\n",
    "prediction_dev = lr.predict(dev_tf_vector)\n",
    "prediction_list = prediction_dev.tolist()\n",
    "save_dev_label(prediction_list, DEV_LABEL_TFIDF_PREDICTION_PATH)\n",
    "prediction_test = lr.predict(test_tf_vector)\n",
    "prediction_list_test = prediction_test.tolist()\n",
    "save_test_label(prediction_list_test, TEST_LABEL_TFIDF_PREDICTION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "DEV_LABEL_TFIDF_PREDICTION_PATH = 'data/final/dev/nb/prediction-tfidf.json'\n",
    "TEST_LABEL_TFIDF_PREDICTION_PATH = 'data/final/test/nb/test-output-tfidf.json'\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_tf_vector, train_label)\n",
    "prediction_dev = nb.predict(dev_tf_vector)\n",
    "prediction_list = prediction_dev.tolist()\n",
    "save_dev_label(prediction_list, DEV_LABEL_TFIDF_PREDICTION_PATH)\n",
    "prediction_test = nb.predict(test_tf_vector)\n",
    "prediction_list_test = prediction_test.tolist()\n",
    "save_test_label(prediction_list_test, TEST_LABEL_TFIDF_PREDICTION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

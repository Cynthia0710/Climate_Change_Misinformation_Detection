{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from raw data\n",
    "import json\n",
    "\n",
    "TRAIN_RUMOUR_JSON_PATH = 'data/raw/train.json'\n",
    "DEV_JSON_PATH = 'data/raw/dev.json'\n",
    "TEST_JSON_PATH = 'data/raw/test-unlabelled.json'\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, 'r') as json_file:\n",
    "        data = json.loads(json_file.read())\n",
    "    return data\n",
    "  \n",
    "train_rumour_json_data = read_json(TRAIN_RUMOUR_JSON_PATH)\n",
    "dev_json_data = read_json(DEV_JSON_PATH)\n",
    "test_json_data = read_json(TEST_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary to list with same format\n",
    "def get_data_list(data):\n",
    "    data_list = []\n",
    "    for key, value in data.items():\n",
    "        text = value['text'].replace('\\n', ' ')\n",
    "        text = text.replace('\\u00A0', '')\n",
    "        data_list.append(text)\n",
    "    return data_list\n",
    "\n",
    "train_rumour_list = get_data_list(train_rumour_json_data)\n",
    "dev_list = get_data_list(dev_json_data)\n",
    "test_list = get_data_list(test_json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train list and train label\n",
    "train_list = []\n",
    "train_label = []\n",
    "for i in train_rumour_list:\n",
    "    train_list.append(i)\n",
    "    train_label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing (punctuation/special punctuation/single letter/lemma)\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def clean_text(text, tt, lemmatizer, punc, chinesepunc, letter, stopword_list):\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def lemma_process(word_token, lemmatizer):\n",
    "        word_token_pos_tag = nltk.pos_tag([word_token])\n",
    "        for word_token, pos_tag in word_token_pos_tag:\n",
    "            word_token_lower = word_token.lower()\n",
    "            wordnet_pos = get_wordnet_pos(pos_tag)\n",
    "            if wordnet_pos == None:\n",
    "                word_token_lemma = lemmatizer.lemmatize(word_token_lower, wordnet.NOUN)\n",
    "            else:\n",
    "                word_token_lemma = lemmatizer.lemmatize(word_token_lower, wordnet_pos)\n",
    "        return word_token_lemma     \n",
    "\n",
    "    def get_new_text(text, tt, lemmatizer, punc, chinesepunc, letter, stopword_list):\n",
    "        new_word_list = []\n",
    "        new_text = \"\"\n",
    "        for word in tt.tokenize(text.lower()):\n",
    "            if (word not in punc) and (word not in chinesepunc) and (word not in stopword_list) and (word not in letter):\n",
    "                word_lemma = lemma_process(word, lemmatizer)\n",
    "                new_word_list.append(word_lemma)\n",
    "        new_text = \" \".join(new_word_list)\n",
    "        return new_text\n",
    "    \n",
    "    new_text = get_new_text(text, tt, lemmatizer, punc, chinesepunc, letter, stopword_list)\n",
    "    \n",
    "    return new_text\n",
    "    \n",
    "tt = TweetTokenizer()\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        \n",
    "punc = string.punctuation\n",
    "chinesepunc = ['“', '”', '‘', '’', '–', '—', '...', '‐', '\\u200b', '.\\u2009.\\u2009.', '\\uf0b7', '\\uf020', '\\u200e', '\\u2066', \n",
    "                '\\u2069', '..', '. .', '…']\n",
    "letter = ['a', 'b', 'c', 'd', \"e\", \"f\", 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', \n",
    "          'x', 'y', 'z'] \n",
    "stopword_list = set(stopwords.words('english'))\n",
    "\n",
    "preprocessed_train_list = []\n",
    "preprocessed_dev_list = []\n",
    "preprocessed_test_list = []\n",
    "\n",
    "for train in train_list:\n",
    "    new_train = clean_text(train, tt, lemmatizer, punc, chinesepunc, letter, stopword_list)\n",
    "    preprocessed_train_list.append(new_train)\n",
    "for dev in dev_list:\n",
    "    new_dev = clean_text(dev, tt, lemmatizer, punc, chinesepunc, letter, stopword_list)\n",
    "    preprocessed_dev_list.append(new_dev)\n",
    "for test in test_list:\n",
    "    new_test = clean_text(test, tt, lemmatizer, punc, chinesepunc, letter, stopword_list)\n",
    "    preprocessed_test_list.append(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 1), tokenizer=tt.tokenize)\n",
    "train_count_vector = cv.fit_transform(preprocessed_train_list)\n",
    "dev_count_vector = cv.transform(preprocessed_dev_list)\n",
    "test_count_vector = cv.transform(preprocessed_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer + TfidfTransformer = TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf = TfidfTransformer()\n",
    "train_tf_vector = tf.fit_transform(train_count_vector)\n",
    "dev_tf_vector = tf.transform(dev_count_vector)\n",
    "test_tf_vector = tf.transform(test_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dev label and test label\n",
    "def save_dev_label(prediction_list, path):\n",
    "    dev_total_labels_dict = dict()\n",
    "    num = 0\n",
    "    for dev_label in prediction_list:\n",
    "        label_dict = dict()\n",
    "        label_dict['label'] = dev_label\n",
    "        dev_total_labels_dict['dev-'+str(num)] = label_dict\n",
    "        num = num + 1\n",
    "    json_str = json.dumps(dev_total_labels_dict)\n",
    "    with open(path, 'w') as json_file:\n",
    "        json_file.write(json_str)\n",
    "        \n",
    "def save_test_label(prediction_list, path):\n",
    "    test_total_labels_dict = dict()\n",
    "    num = 0\n",
    "    for test_label in prediction_list:\n",
    "        label_dict = dict()\n",
    "        label_dict['label'] = test_label\n",
    "        test_total_labels_dict['test-'+str(num)] = label_dict\n",
    "        num = num + 1\n",
    "    json_str = json.dumps(test_total_labels_dict)\n",
    "    with open(path, 'w') as json_file:\n",
    "        json_file.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one class svm\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "DEV_LABEL_TFIDF_PREDICTION_PATH = 'data/dev/oneclasssvm/prediction-tfidf.json'\n",
    "TEST_LABEL_TFIDF_PREDICTION_PATH = 'data/test/oneclasssvm/test-output-tfidf.json'\n",
    "\n",
    "one_svm = OneClassSVM(gamma='scale', nu=0.01)\n",
    "one_svm.fit(train_tf_vector, train_label)\n",
    "prediction_dev = one_svm.predict(dev_tf_vector)\n",
    "prediction_list = prediction_dev.tolist()\n",
    "change_prediction_list = []\n",
    "for prediction in prediction_list:\n",
    "    if prediction == (-1):\n",
    "        prediction = 0\n",
    "        change_prediction_list.append(prediction)\n",
    "    else:\n",
    "        change_prediction_list.append(prediction)\n",
    "save_dev_label(change_prediction_list, DEV_LABEL_TFIDF_PREDICTION_PATH)\n",
    "prediction_test = one_svm.predict(test_tf_vector)\n",
    "prediction_list_test = prediction_test.tolist()\n",
    "change_prediction_list_test = []\n",
    "for prediction in prediction_list_test:\n",
    "    if prediction == (-1):\n",
    "        prediction = 0\n",
    "        change_prediction_list_test.append(prediction)\n",
    "    else:\n",
    "        change_prediction_list_test.append(prediction)\n",
    "save_test_label(change_prediction_list_test, TEST_LABEL_TFIDF_PREDICTION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
